{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, nvec_s: int, nvec_u: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(nvec_s, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, nvec_u)\n",
    "\n",
    "\n",
    "    def forward(self, x, deterministic = False):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        dist = torch.distributions.Categorical(logits=x)\n",
    "        if deterministic:\n",
    "            action = torch.argmax(x)\n",
    "            return action\n",
    "        action = dist.sample()\n",
    "        entropy = dist.entropy()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob, entropy\n",
    "    \n",
    "    def evaluate_actions(self,states, actions):\n",
    "        x = F.tanh(self.fc1(states))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        dist = torch.distributions.Categorical(logits=x)\n",
    "        log_prob = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_prob, entropy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden,n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "\n",
    "    def __init__(self, env:VecEnv, capacity, gamma, gae_lambda, device:torch.device, num_envs):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.n_actions = env.action_space.n \n",
    "        self.n_states = env.observation_space.shape[0]\n",
    "\n",
    "        self.states = np.zeros((capacity,num_envs, self.n_states), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.rewards = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.values = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.log_probs = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.entropy = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.advantages = np.zeros((capacity,num_envs), dtype=np.float32)\n",
    "        self.retuns = np.zeros((capacity,num_envs), dtype=np.float32) \n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def push(self, state:np.ndarray, action, reward, done, value:torch.Tensor, log_prob:torch.Tensor, entropy):\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.dones[self.position] = done\n",
    "        self.values[self.position] = value.clone().cpu().numpy().flatten()\n",
    "        self.log_probs[self.position] = log_prob.clone().cpu().numpy().flatten()\n",
    "        self.entropy[self.position] = entropy\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def compute_advantages_and_returns(self, last_value:torch.Tensor, dones):\n",
    "        last_value = last_value.cpu().numpy().flatten()\n",
    "        last_gae = 0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            if step == len(self.rewards) - 1:\n",
    "                next_value = last_value\n",
    "                next_non_terminal = 1 - dones\n",
    "            else:\n",
    "                next_value = self.values[step + 1]\n",
    "                next_non_terminal = 1 - self.dones[step + 1]   \n",
    "            \n",
    "            delta = self.rewards[step] + self.gamma * next_value * next_non_terminal - self.values[step]\n",
    "            last_gae = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae\n",
    "            self.advantages[step]= last_gae\n",
    "\n",
    "        self.returns = self.advantages + self.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: VecEnv,\n",
    "        lr,  # learning rate\n",
    "        gamma,  # discount factor\n",
    "        gae_lambda,  # Generalized Advantage Estimation lambda\n",
    "        max_steps,  # max steps for training\n",
    "        n_steps,  # number of steps to run before updating\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.num_envs = env.num_envs\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.max_steps = max_steps\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "\n",
    "        self.rollout_buffer = RolloutBuffer(\n",
    "            env, n_steps, gamma, gae_lambda, device, num_envs=self.num_envs\n",
    "        )\n",
    "        self.policy_net = PolicyNet(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.value_net = ValueNet(env.observation_space.shape[0], 128)\n",
    "        self.optimizer_policy = RMSprop(self.policy_net.parameters(), lr=self.lr, eps=1e-5)\n",
    "        self.optimizer_value = RMSprop(self.value_net.parameters(), lr=self.lr, eps=1e-5)\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.last_state = self.env.reset()\n",
    "\n",
    "        # stats\n",
    "        self.episodes = 0\n",
    "        self.total_rewards = np.zeros(self.env.num_envs)\n",
    "        self.mean_episode_reward = 0\n",
    "\n",
    "    def rollout(self):\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, entropy = self.policy_net(\n",
    "                    torch.from_numpy(self.last_state).float().to(self.device)\n",
    "                )\n",
    "                value = self.value_net(\n",
    "                    torch.tensor(self.last_state).float().to(self.device)\n",
    "                )\n",
    "\n",
    "            next_state, rewards, dones, infos = self.env.step(action.detach().numpy())\n",
    "\n",
    "            # check if done is because of truncation\n",
    "            for idx, done in enumerate(dones):\n",
    "                if (\n",
    "                    done\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = infos[idx][\"terminal_observation\"]\n",
    "                    with torch.no_grad():\n",
    "                        terminal_value = self.value_net(torch.tensor(terminal_obs).float().to(self.device))  # type: ignore[arg-type]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            self.rollout_buffer.push(\n",
    "                self.last_state, action, rewards, done, value, log_prob, entropy\n",
    "            )\n",
    "\n",
    "            self.total_rewards += rewards\n",
    "            self.total_steps += 1\n",
    "            self.pbar.update(1)\n",
    "\n",
    "            if dones[0]:\n",
    "                self.last_state = self.env.reset()\n",
    "                self.episodes += 1\n",
    "\n",
    "                if self.episodes % 100 == 0:\n",
    "\n",
    "                    self.mean_episode_reward = (\n",
    "                        self.total_rewards[0]\n",
    "                    ) / self.episodes\n",
    "                    self.pbar.set_description(\n",
    "                        f\"Reward: {self.mean_episode_reward :.3f}\"\n",
    "                    )\n",
    "                    # self.writer.add_scalar(\"reward\", self.total_rewards, self.episodes)\n",
    "                    self.episodes = 0\n",
    "                    self.total_rewards = np.zeros(self.env.num_envs)\n",
    "\n",
    "            self.last_state = next_state\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_value = self.value_net(\n",
    "                torch.tensor(self.last_state).float().to(self.device)\n",
    "            )\n",
    "\n",
    "        self.rollout_buffer.compute_advantages_and_returns(last_value, dones)\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        log_prob, entropy = self.policy_net.evaluate_actions(\n",
    "            torch.tensor(self.rollout_buffer.states).float().to(self.device),\n",
    "            torch.tensor(self.rollout_buffer.actions).to(self.device),\n",
    "        )\n",
    "\n",
    "        values = self.value_net(\n",
    "            torch.tensor(self.rollout_buffer.states).float().to(self.device)\n",
    "        ).squeeze()\n",
    "\n",
    "        # advantages = self.rollout_buffer.advantages\n",
    "        \n",
    "        advantages = (self.rollout_buffer.advantages - self.rollout_buffer.advantages.mean())/ (self.rollout_buffer.advantages.std() + 1e-5)\n",
    "\n",
    "        self.policy_loss = -torch.mean(\n",
    "            torch.from_numpy(advantages) * log_prob\n",
    "        )\n",
    "\n",
    "        self.entropy_loss = -torch.mean(entropy)\n",
    "        self.policy_loss = self.policy_loss + 0.0 * self.entropy_loss\n",
    "\n",
    "        self.value_loss = F.mse_loss(\n",
    "            torch.from_numpy(self.rollout_buffer.returns), values\n",
    "        )\n",
    "\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        self.policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "        self.optimizer_value.zero_grad()\n",
    "        self.value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n",
    "        self.optimizer_value.step()\n",
    "\n",
    "    def train(self):\n",
    "        self.pbar = tqdm(total=self.max_steps, position=0, leave=True)\n",
    "        # self.writer = SummaryWriter(log_dir=\"runs/reinforce_logs/REINFORCE_BASELINE_2\")\n",
    "\n",
    "        while self.total_steps < self.max_steps:\n",
    "            self.rollout_buffer.reset()\n",
    "            self.rollout()\n",
    "            self.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    # env = gym.wrappers.time_limit.TimeLimit(env, max_episode_steps=500)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(make_env, n_envs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(\n",
    "    env=env,\n",
    "    lr=0.0001,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.9,\n",
    "    max_steps=500000,\n",
    "    n_steps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reward: 9.430:  17%|█▋        | 84190/500000 [1:03:01<5:11:15, 22.27it/s]\n",
      "Reward: 64.770:  29%|██▉       | 144667/500000 [57:40<2:21:39, 41.80it/s]\n",
      "Reward: 55.200:  36%|███▌      | 178688/500000 [47:22<1:25:11, 62.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 139\u001b[0m, in \u001b[0;36mA2C.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn()\n",
      "Cell \u001b[1;32mIn[34], line 43\u001b[0m, in \u001b[0;36mA2C.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 43\u001b[0m         action, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(\n\u001b[0;32m     47\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     50\u001b[0m     next_state, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, x, deterministic)\u001b[0m\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(x)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n\u001b[1;32m---> 17\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\n\u001b[0;32m     19\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\distributions\\categorical.py:131\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_shape, torch\u001b[38;5;241m.\u001b[39mSize):\n\u001b[0;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m--> 131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m    132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs_2d, sample_shape\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\distributions\\utils.py:126\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[1;34m(self, instance, obj_type)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 126\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, value)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\distributions\\categorical.py:100\u001b[0m, in \u001b[0;36mCategorical.probs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;129m@lazy_property\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogits_to_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\distributions\\utils.py:89\u001b[0m, in \u001b[0;36mlogits_to_probs\u001b[1;34m(logits, is_binary)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_binary:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\torch\\nn\\functional.py:1885\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1883\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy_net(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(obs)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice), deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 15\u001b[0m     obs, reward, terminated,  truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    187\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\rl\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reward: 55.200:  36%|███▌      | 178688/500000 [10:02<18:36, 287.69it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\",\n",
    "               render_mode='human'\n",
    "               )\n",
    "\n",
    "n_episodes = 100\n",
    "for _ in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        with torch.no_grad():\n",
    "            action = agent.policy_net(torch.from_numpy(obs).float().to(agent.device), deterministic=True).item()\n",
    "            obs, reward, terminated,  truncated, info = env.step(action)\n",
    "            env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
